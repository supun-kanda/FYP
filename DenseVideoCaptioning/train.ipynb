{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from opt import *\n",
    "from data_provider import *\n",
    "from model import * \n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.path.insert(0, './densevid_eval-master')\n",
    "sys.path.insert(0, './densevid_eval-master/coco-caption')\n",
    "#from evaluator import *\n",
    "#from evaluator_efficient import *\n",
    "from evaluator_old import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKey(item):\n",
    "    return item['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss evaluation\n",
    "\"\"\"\n",
    "def evaluation(options, data_provision, sess, inputs, t_loss):\n",
    "    val_loss_list = []\n",
    "    val_proposal_loss_list =[]\n",
    "    val_caption_loss_list = []\n",
    "    val_count = min(data_provision.get_size('val'), options['loss_eval_num'])\n",
    "    batch_size = options['batch_size']\n",
    "    \n",
    "    count = 0\n",
    "    for batch_data in data_provision.iterate_batch('val', batch_size):\n",
    "        print('Evaluating batch: #%d'%count)\n",
    "        count += 1\n",
    "        feed_dict = {inputs['rnn_drop']:0.}\n",
    "        for key, value in batch_data.items():\n",
    "            if key not in inputs:\n",
    "                continue\n",
    "            feed_dict[inputs[key]] = value\n",
    "\n",
    "        loss, proposal_loss, caption_loss = sess.run(\n",
    "                    t_loss,\n",
    "                    feed_dict=feed_dict)\n",
    "        val_loss_list.append(loss * batch_data['caption'].shape[0])\n",
    "        val_proposal_loss_list.append(proposal_loss * batch_data['caption'].shape[0])\n",
    "        val_caption_loss_list.append(caption_loss * batch_data['caption'].shape[0])\n",
    "\n",
    "        if count >= val_count:\n",
    "            break\n",
    "    print('val_loss_list =', val_loss_list)\n",
    "    ave_val_loss = sum(val_loss_list) / float(val_count)\n",
    "    ave_proposal_val_loss = sum(val_proposal_loss_list) / float(val_count)\n",
    "    ave_caption_val_loss = sum(val_caption_loss_list) / float(val_count)\n",
    "    return ave_val_loss, ave_proposal_val_loss, ave_caption_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate batch data and corresponding mask data for the input\n",
    "\"\"\"\n",
    "def process_batch_data(batch_data, max_length):\n",
    "    dim = batch_data[0].shape[1]\n",
    "\n",
    "    out_batch_data = np.zeros(shape=(len(batch_data), max_length, dim), dtype='float32')\n",
    "    out_batch_data_mask = np.zeros(shape=(len(batch_data), max_length), dtype='int32')\n",
    "\n",
    "    for i, data in enumerate(batch_data):\n",
    "        effective_len = min(max_length, data.shape[0])\n",
    "        out_batch_data[i, :effective_len, :] = data[:effective_len]\n",
    "        out_batch_data_mask[i, :effective_len] = 1\n",
    "\n",
    "    out_batch_data = np.asarray(out_batch_data, dtype='float32')\n",
    "    out_batch_data_mask = np.asarray(out_batch_data_mask, dtype='int32')\n",
    "\n",
    "    return out_batch_data, out_batch_data_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metric_greedy(options, data_provision, sess, proposal_inputs, caption_inputs, proposal_outputs, caption_outputs):\n",
    "    print('Evaluating caption scores ...')\n",
    "\n",
    "    word2ix = options['vocab']\n",
    "    ix2word = {ix:word for word,ix in word2ix.items()}\n",
    "    \n",
    "    # output json data for evaluation\n",
    "    out_data = {}\n",
    "    out_data['version'] = 'VERSION 1.0'\n",
    "    out_data['external_data'] = {'used':False, 'details': ''}\n",
    "    out_data['results'] = {}\n",
    "    results = {}\n",
    "    \n",
    "    count = 0\n",
    "    batch_size = options['eval_batch_size']    # default batch size to evaluate\n",
    "    assert batch_size == 1\n",
    "    \n",
    "    eval_num = batch_size*options['metric_eval_num']\n",
    "    print('Will evaluate %d samples'%eval_num)\n",
    "\n",
    "    val_ids = data_provision.get_ids('val')[:eval_num]\n",
    "    anchors = data_provision.get_anchors()\n",
    "    localizaitons = data_provision.get_localization()\n",
    "\n",
    "    for batch_data in data_provision.iterate_batch('val', batch_size):\n",
    "        print('\\nProcessed %d-th batch \\n'%count)\n",
    "        vid = val_ids[count]\n",
    "        print('video id: %s'%vid)\n",
    "\n",
    "        proposal_score_fw, proposal_score_bw, rnn_outputs_fw, rnn_outputs_bw = sess.run([proposal_outputs['proposal_score_fw'], proposal_outputs['proposal_score_bw'], proposal_outputs['rnn_outputs_fw'], proposal_outputs['rnn_outputs_bw']], feed_dict={proposal_inputs['video_feat_fw']:batch_data['video_feat_fw'], proposal_inputs['video_feat_bw']:batch_data['video_feat_bw']})\n",
    "        \n",
    "        feat_len = batch_data['video_feat_fw'][0].shape[0]\n",
    "        duration = localizaitons['val'][vid]['duration']\n",
    "        \n",
    "        '''calculate final score by summarizing forward score and backward score\n",
    "        '''\n",
    "        proposal_score = np.zeros((feat_len, options['num_anchors']))\n",
    "        proposal_infos = []\n",
    "\n",
    "        \n",
    "        for i in range(feat_len):\n",
    "            pre_start = -1.\n",
    "            for j in range(options['num_anchors']):\n",
    "                forward_score = proposal_score_fw[i,j]\n",
    "                # calculate time stamp\n",
    "                end = (float(i+1)/feat_len)*duration\n",
    "                start = end-anchors[j]\n",
    "                start = max(0., start)\n",
    "\n",
    "                if start == pre_start:\n",
    "                    continue\n",
    "\n",
    "                # backward\n",
    "                end_bw = duration - start\n",
    "                i_bw = min(int(round((end_bw/duration)*feat_len)-1), feat_len-1)\n",
    "                i_bw = max(i_bw, 0)\n",
    "                backward_score = proposal_score_bw[i_bw,j]\n",
    "\n",
    "                proposal_score[i,j] = forward_score*backward_score\n",
    "\n",
    "                hidden_feat = np.concatenate([rnn_outputs_fw[i], rnn_outputs_bw[i_bw]], axis=-1)\n",
    "                    \n",
    "                \n",
    "                proposal_feats = batch_data['video_feat_fw'][0][feat_len-1-i_bw:i+1]\n",
    "                proposal_infos.append({'timestamp':[start, end], 'score': proposal_score[i,j], 'event_hidden_feats': hidden_feat, 'proposal_feats': proposal_feats})\n",
    "                            \n",
    "                pre_start = start\n",
    "        \n",
    "        # add the largest proposal\n",
    "        hidden_feat = np.concatenate([rnn_outputs_fw[feat_len-1], rnn_outputs_bw[feat_len-1]], axis=-1)\n",
    "            \n",
    "        \n",
    "        proposal_feats = batch_data['video_feat_fw'][0]\n",
    "        proposal_infos.append({'timestamp':[0., duration], 'score': 1., 'event_hidden_feats': hidden_feat, 'proposal_feats': proposal_feats})\n",
    "        \n",
    "\n",
    "        proposal_infos = sorted(proposal_infos, key=getKey, reverse=True)\n",
    "        proposal_infos = proposal_infos[:options['max_proposal_num']]\n",
    "\n",
    "        print('Number of proposals: %d'%len(proposal_infos))\n",
    "\n",
    "        # \n",
    "        event_hidden_feats = [item['event_hidden_feats'] for item in proposal_infos]\n",
    "        proposal_feats = [item['proposal_feats'] for item in proposal_infos]\n",
    "\n",
    "        \n",
    "        event_hidden_feats = np.array(event_hidden_feats, dtype='float32')\n",
    "        proposal_feats, _ = process_batch_data(proposal_feats, options['max_proposal_len'])\n",
    "\n",
    "        # run session to get word ids\n",
    "        word_ids = sess.run(caption_outputs['word_ids'], feed_dict={caption_inputs['event_hidden_feats']: event_hidden_feats, caption_inputs['proposal_feats']: proposal_feats})\n",
    "        \n",
    "        \n",
    "        sentences = [[ix2word[i] for i in ids] for ids in word_ids]\n",
    "        sentences = [sentence[1:] for sentence in sentences]\n",
    "        \n",
    "        # remove <END> word\n",
    "        out_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if '<END>' in sentence:\n",
    "                sentence = sentence[:sentence.index('<END>')]\n",
    "            out_sentences.append(' '.join(sentence))\n",
    "\n",
    "        \n",
    "        print('Output sentences: ')\n",
    "        for out_sentence in out_sentences:\n",
    "            print(out_sentence)\n",
    "\n",
    "        result = [{'timestamp': proposal['timestamp'], 'sentence': out_sentences[i]} for i, proposal in enumerate(proposal_infos)]\n",
    "                    \n",
    "        results[vid] = result\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count >= eval_num:\n",
    "            break\n",
    "\n",
    "    out_data['results'] = results\n",
    "    \n",
    "    resFile = 'results/%d/temp_results.json'%options['train_id']\n",
    "    root_folder = os.path.dirname(resFile)\n",
    "    if not os.path.exists(root_folder):\n",
    "        os.makedirs(root_folder)\n",
    "\n",
    "    print('Saving result json file ...')\n",
    "    with open(resFile, 'w') as fid:\n",
    "        json.dump(out_data, fid)\n",
    "\n",
    "    # Call evaluator\n",
    "    \n",
    "    evaluator = ANETcaptions(ground_truth_filenames=['densevid_eval-master/data/val_1.json', 'densevid_eval-master/data/val_2.json'],\n",
    "                             prediction_filename=resFile,\n",
    "                             tious=options['tiou_measure'],\n",
    "                             max_proposals=1000,\n",
    "                             verbose=False)\n",
    "    evaluator.evaluate()\n",
    "\n",
    "    # Output the results\n",
    "    for i, tiou in enumerate(options['tiou_measure']):\n",
    "        print('-' * 80)\n",
    "        print('tIoU: %.2f'%tiou)\n",
    "        print('-' * 80)\n",
    "        for metric in evaluator.scores:\n",
    "            score = evaluator.scores[metric][i]\n",
    "            print('| %s: %2.4f'%(metric, 100*score))\n",
    "\n",
    "    # Print the averages\n",
    "    print('-' * 80)\n",
    "    print('Average across all tIoUs')\n",
    "    print('-' * 80)\n",
    "    avg_scores = {}\n",
    "    for metric in evaluator.scores:\n",
    "        score = evaluator.scores[metric]\n",
    "        avg_score = 100 * sum(score) / float(len(score))\n",
    "        avg_scores[metric] = avg_score\n",
    "    \n",
    "    # print output evaluation scores\n",
    "    fid = open('results/%d/score_history.txt'%options['train_id'], 'a')\n",
    "    for metric, score in avg_scores.items():\n",
    "        print('%s: %.4f'%(metric, score))\n",
    "        # also write to a temp file\n",
    "        fid.write('%s: %.4f\\n'%(metric, score))\n",
    "    fid.write('\\n')\n",
    "    fid.close()\n",
    "\n",
    "    combined_score = avg_scores['METEOR']\n",
    "    \n",
    "    return avg_scores, combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'sakila'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = default_options()\n",
    "work_dir = os.path.dirname(options['status_file'])\n",
    "if os.path.exists(work_dir) :\n",
    "    print('work_dir %s exists! Pls check it.'%work_dir)\n",
    "else:\n",
    "    os.makedirs(work_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth=True\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(options['gpu_id'])[1:-1]\n",
    "sess = tf.InteractiveSession(config=sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Load data ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_provision = DataProvision(options)\n",
    "\n",
    "batch_size = options['batch_size']\n",
    "max_epochs = options['max_epochs']\n",
    "init_epoch = options['init_epoch']\n",
    "lr_init = options['learning_rate']\n",
    "status_file = options['status_file']\n",
    "lr = lr_init\n",
    "lr_decay_factor = options['lr_decay_factor']\n",
    "n_epoch_to_decay = options['n_epoch_to_decay'] # when to decay the lr\n",
    "next_epoch_to_decay = n_epoch_to_decay.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable proposal_module/video_encoder_fw/rnn/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in __call__\n    scope=scope)\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 524, in __call__\n    output, new_state = self._cell(inputs, state, scope)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-205076d96974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build model for training ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCaptionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mt_proposal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proposal_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/GIT/DenseVideoCaptioning/model.pyc\u001b[0m in \u001b[0;36mbuild_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m                 )\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    711\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\u001b[0m\n\u001b[1;32m   2603\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2604\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2606\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2436\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m       original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2438\u001b[0;31m           pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2439\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2440\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_loop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m-> 2388\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m       \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           skip_conditionals=True)\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m_rnn_step\u001b[0;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0mnew_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    522\u001b[0m         self._input_keep_prob < 1):\n\u001b[1;32m    523\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_keep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     if (not isinstance(self._output_keep_prob, float) or\n\u001b[1;32m    526\u001b[0m         self._output_keep_prob < 1):\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    336\u001b[0m       \u001b[0;31m# i = input_gate, j = new_input, f = forget_gate, o = output_gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m       lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True,\n\u001b[0;32m--> 338\u001b[0;31m                             scope=scope)\n\u001b[0m\u001b[1;32m    339\u001b[0m       i, j, f, o = array_ops.split(\n\u001b[1;32m    340\u001b[0m           value=lstm_matrix, num_or_size_splits=4, axis=1)\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.pyc\u001b[0m in \u001b[0;36m_linear\u001b[0;34m(args, output_size, bias, bias_start, scope)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mouter_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     weights = vs.get_variable(\n\u001b[0;32m--> 747\u001b[0;31m         \"weights\", [total_arg_size, output_size], dtype=dtype)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable proposal_module/video_encoder_fw/rnn/lstm_cell/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 747, in _linear\n    \"weights\", [total_arg_size, output_size], dtype=dtype)\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 338, in __call__\n    scope=scope)\n  File \"/home/supunK/anaconda3/envs/dense/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 524, in __call__\n    output, new_state = self._cell(inputs, state, scope)\n"
     ]
    }
   ],
   "source": [
    "n_iters_per_epoch = data_provision.get_size('train') // batch_size\n",
    "eval_in_iters = n_iters_per_epoch // options['n_eval_per_epoch']\n",
    "\n",
    "#############################################\n",
    "# build model #\n",
    "\n",
    "print('Build model for training ...')\n",
    "model = CaptionModel(options)\n",
    "inputs, outputs = model.build_train()\n",
    "t_loss = outputs['loss']\n",
    "t_proposal_loss = outputs['proposal_loss']\n",
    "t_caption_loss = outputs['caption_loss']\n",
    "t_loss_list = [t_loss, t_proposal_loss, t_caption_loss]\n",
    "t_reg_loss = outputs['reg_loss']\n",
    "t_n_proposals = outputs['n_proposals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if options['evaluate_metric']:\n",
    "        print('Build model for evaluating metric ...')\n",
    "        proposal_inputs, proposal_outputs = model.build_proposal_inference(reuse=True)\n",
    "        caption_inputs, caption_outputs = model.build_caption_greedy_inference(reuse=True)\n",
    "        t_proposal_score_fw = proposal_outputs['proposal_score_fw']\n",
    "        t_proposal_score_bw = proposal_outputs['proposal_score_bw']\n",
    "        t_rnn_outputs_fw = proposal_outputs['rnn_outputs_fw']\n",
    "        t_rnn_outputs_bw = proposal_outputs['rnn_outputs_bw']\n",
    "        t_word_ids = caption_outputs['word_ids']\n",
    "    #############################################\n",
    "    \n",
    "    t_summary = tf.summary.merge_all()\n",
    "    t_lr = tf.placeholder(tf.float32)\n",
    "\n",
    "    \n",
    "    if options['solver'] == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=t_lr)\n",
    "    elif options['solver'] == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=t_lr)\n",
    "    elif options['solver'] == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=t_lr, momentum=options['momentum'])\n",
    "    elif options['solver'] == 'adadelta':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=t_lr)\n",
    "    else:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=t_lr)\n",
    "    \n",
    "    # get trainable variable list\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    if not options['train_proposal']:\n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='caption_module')\n",
    "    if not options['train_caption']:\n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='proposal_module')\n",
    "    if not options['train_proposal'] and not options['train_caption']:\n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "\n",
    "    # gradient clipping option\n",
    "    if options['clip_gradient_norm'] < 0:\n",
    "        train_op = optimizer.minimize(t_loss + options['reg'] * t_reg_loss, var_list=trainable_vars)\n",
    "    else:\n",
    "        gvs = optimizer.compute_gradients(t_loss + options['reg'] * t_reg_loss, var_list=trainable_vars)\n",
    "        clip_grad_var = [(tf.clip_by_norm(grad, options['clip_gradient_norm']), var) for grad, var in gvs]\n",
    "        train_op = optimizer.apply_gradients(clip_grad_var)\n",
    "\n",
    "    # save summary data\n",
    "    train_summary_writer = tf.summary.FileWriter(os.path.dirname(options['status_file']), sess.graph)\n",
    "\n",
    "    # initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "    ## test model variable shape\n",
    "    if 'print_debug' in options.keys() and options['print_debug']:\n",
    "        print('*********** Variable Shape *************')\n",
    "        for v in tf.trainable_variables():\n",
    "            print('%s:'%v.name)\n",
    "            print(v.get_shape())\n",
    "\n",
    "        if 'test_tensors' in options:\n",
    "            print('********** Tensor Shape ************')\n",
    "            tf_graph = tf.get_default_graph()\n",
    "            for t_name in options['test_tensors']:\n",
    "                t = tf_graph.get_tensor_by_name('%s:0'%t_name)\n",
    "                print('%s: '%t_name)\n",
    "                print(t.get_shape())\n",
    "\n",
    "\n",
    "    # for saving and restoring checkpoints during training\n",
    "    saver = tf.train.Saver(max_to_keep=200, write_version=1)\n",
    "\n",
    "    # initialize model from a given checkpoint path\n",
    "    if options['init_from']:\n",
    "        print('Init model from %s'%options['init_from'])\n",
    "        pre_status = json.load(open(os.path.join(os.path.dirname(options['init_from']), 'status.json')))\n",
    "        pre_options = pre_status['options']\n",
    "        restore_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "\n",
    "        if options['init_module'] == 'proposal':\n",
    "            print('Restoring parameters only for proposal module')\n",
    "            restore_vars = [v for v in restore_vars if v.name.startswith('proposal_module/')]\n",
    "        elif options['init_module'] == 'caption':\n",
    "            print('Restoring parameters only for caption module')\n",
    "            restore_vars = [v for v in restore_vars if v.name.startswith('caption_module/')]\n",
    "        elif options['init_module'] == 'all':\n",
    "            pass\n",
    "            \n",
    "        # for restoring from another graph (contain different structure) at the beginning\n",
    "        saver_part = tf.train.Saver(var_list=restore_vars)\n",
    "        saver_part.restore(sess, options['init_from'])\n",
    "\n",
    "    # save loss/evaluation history\n",
    "    json_worker_status = OrderedDict()\n",
    "    json_worker_status['options'] = options\n",
    "    json_worker_status['history'] = []\n",
    "    json_worker_status['eval_results'] = []\n",
    "    json.dump(json_worker_status, open(options['status_file'], 'w'))\n",
    "\n",
    "\n",
    "    if options['eval_init']:\n",
    "        print('Evaluating the initialized model ...')\n",
    "        val_loss, val_proposal_loss, val_caption_loss = evaluation(options, data_provision, sess, inputs, t_loss_list)\n",
    "        print('loss: %.4f, proposal_loss: %.4f, caption_loss: %.4f'%(val_loss, val_proposal_loss, val_caption_loss))\n",
    "\n",
    "\n",
    "        combined_score = -1   # denote not evaluated\n",
    "        all_scores = -1\n",
    "        if options['evaluate_metric']:\n",
    "            all_scores, combined_score = evaluation_metric_greedy(options, data_provision, sess, proposal_inputs, caption_inputs, proposal_outputs, caption_outputs)\n",
    "            \n",
    "            print('combined score: %.3f'%(combined_score,))\n",
    "\n",
    "\n",
    "    t0 = time.time()\n",
    "    eval_id = 0\n",
    "    train_batch_generator = data_provision.iterate_batch('train', batch_size)\n",
    "    checkpoint_filenames = []\n",
    "    \n",
    "    for epoch in range(init_epoch, max_epochs):\n",
    "        \n",
    "        # manually set when to decay learning rate\n",
    "        if not options['auto_lr_decay']:\n",
    "            if epoch == next_epoch_to_decay:\n",
    "                if len(n_epoch_to_decay) == 0:\n",
    "                    next_epoch_to_decay = -1\n",
    "                else:\n",
    "                    next_epoch_to_decay = n_epoch_to_decay.pop()\n",
    "\n",
    "                print('Decaying learning rate ...')\n",
    "                lr *= lr_decay_factor\n",
    "        \n",
    "        print('epoch: %d/%d, lr: %.1E (%.1E)'%(epoch, max_epochs, lr, lr_init))\n",
    "        for iter in range(n_iters_per_epoch):\n",
    "            batch_data = next(train_batch_generator)\n",
    "            feed_dict = {\n",
    "                t_lr: lr,\n",
    "                inputs['rnn_drop']: options['rnn_drop']\n",
    "            }\n",
    "            for key, value in batch_data.items():\n",
    "                if key not in inputs:\n",
    "                    continue\n",
    "                feed_dict[inputs[key]] = value\n",
    "\n",
    "            _, summary, loss, proposal_loss, caption_loss, reg_loss, n_proposals = sess.run([train_op, t_summary, t_loss, t_proposal_loss, t_caption_loss, t_reg_loss, t_n_proposals], feed_dict=feed_dict)\n",
    "\n",
    "            if 'print_debug' in options and options['print_debug']:\n",
    "                print('n_proposals: %d'%n_proposals)\n",
    "            \n",
    "            if iter == 0 and epoch == init_epoch:\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.9 * smooth_loss + 0.1 * loss\n",
    "            \n",
    "            if iter % options['n_iters_display'] == 0:\n",
    "                print('iter: %d, epoch: %d/%d, \\nlr: %.1E, loss: %.4f, proposal_loss: %.4f, caption_loss: %.4f'%(iter, epoch, max_epochs, lr, loss, proposal_loss, caption_loss))\n",
    "                train_summary_writer.add_summary(summary, iter + epoch * n_iters_per_epoch)\n",
    "                jstatus = OrderedDict()\n",
    "                jstatus['epoch'] = (epoch, max_epochs)\n",
    "                jstatus['iter'] = (iter, n_iters_per_epoch)\n",
    "                jstatus['loss'] = (float(loss), float(smooth_loss), float(reg_loss))\n",
    "                json_worker_status['history'].append(jstatus)\n",
    "\n",
    "\n",
    "            # every 30 secs write once\n",
    "            if (time.time() - t0) / 60.0 > 0.5:\n",
    "                t0 = time.time()\n",
    "                json.dump(json_worker_status, open(status_file, 'w'))\n",
    "            \n",
    "            if (iter + 1) % eval_in_iters == 0:\n",
    "                \n",
    "                print('Evaluating model ...')\n",
    "                val_loss, val_proposal_loss, val_caption_loss = evaluation(options, data_provision, sess, inputs, t_loss_list)\n",
    "                print('loss: %.4f, proposal_loss: %.4f, caption_loss: %.4f'%(val_loss, val_proposal_loss, val_caption_loss))\n",
    "\n",
    "                combined_score = -1   # denote not evaluated\n",
    "                all_scores = -1\n",
    "                if options['evaluate_metric']:\n",
    "                    all_scores, combined_score = evaluation_metric_greedy(options, data_provision, sess, proposal_inputs, caption_inputs, proposal_outputs, caption_outputs)\n",
    "                    \n",
    "                    print('combined score: %.3f'%(combined_score,))\n",
    "\n",
    "                jeval_results = OrderedDict()\n",
    "                jeval_results['loss'] = (val_loss, smooth_loss)\n",
    "                jeval_results['score'] = combined_score\n",
    "                jeval_results['scores'] = all_scores\n",
    "                jeval_results['lr'] = lr\n",
    "                json_worker_status['eval_results'].append(jeval_results)\n",
    "                json.dump(json_worker_status, open(status_file, 'w'))\n",
    "\n",
    "                checkpoint_path = '%sepoch%02d_%.2f_%02d_lr%f%s.ckpt' % (options['ckpt_prefix'], epoch, val_loss, eval_id, lr, options['ckpt_sufix'])\n",
    "                if options['evaluate_metric']:\n",
    "                    checkpoint_path = '%sepoch%02d_%.2f_%02d_lr%f%s.ckpt' % (options['ckpt_prefix'], epoch, combined_score, eval_id, lr, options['ckpt_sufix'])\n",
    "\n",
    "                saver.save(sess, checkpoint_path)\n",
    "                checkpoint_filenames.append(checkpoint_path)\n",
    "                \n",
    "                eval_id = eval_id + 1\n",
    "\n",
    "                # automatically lower learning rate\n",
    "                if options['auto_lr_decay']:\n",
    "                    # review val loss history or score history\n",
    "                    eval_results = json_worker_status['eval_results']\n",
    "                    view_end_eval_id = eval_id\n",
    "                    view_start_eval_id = view_end_eval_id - options['n_eval_observe']\n",
    "                    view_start_epoch_id = (view_end_eval_id + init_epoch*options['n_eval_per_epoch'] - options['n_eval_observe']) // options['n_eval_per_epoch']\n",
    "                    \n",
    "\n",
    "                    review_results = [result['loss'][0] for result in eval_results[view_start_eval_id:view_end_eval_id]]\n",
    "                    if options['evaluate_metric']:\n",
    "                        review_results = [result['score'] for result in eval_results[view_start_eval_id:view_end_eval_id]] \n",
    "                    \n",
    "                    if view_start_eval_id >= 0:\n",
    "                        if options['evaluate_metric'] and review_results.index(max(review_results)) == 0:\n",
    "                            # go back to the state of view_start_eval_id, and lower learning rate  \n",
    "                            print('Init model from %s ...'%checkpoint_filenames[view_start_eval_id])\n",
    "                            saver.restore(sess, checkpoint_filenames[view_start_eval_id])\n",
    "                            print('Decaying learning rate ...')\n",
    "                            lr *= lr_decay_factor\n",
    "                            if lr < options['min_lr']:\n",
    "                                print('Reach minimum learning rate. Done training.')\n",
    "                                return\n",
    "                        elif not options['evaluate_metric'] and review_results.index(min(review_results)) == 0:\n",
    "                            # go back to the state of view_start_eval_id, and lower learning rate\n",
    "                            print('Init model from %s ...'%checkpoint_filenames[view_start_eval_id])\n",
    "                            saver.restore(sess, checkpoint_filenames[view_start_eval_id])\n",
    "                            print('Decaying learning rate ...')\n",
    "                            lr *= lr_decay_factor\n",
    "                            if lr < options['min_lr']:\n",
    "                                print('Reach minimum learning rate. Done training.')\n",
    "                                return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
